# Python-Machine-Learning_Potential_Customer
Design a predictive model to determine the potential customers for the next product's promotion for Ads Company

1. Create a feature name "Potential Customer" (for model training purposes) ‚ÄºÔ∏è \
  ‚û°Ô∏è Set the Potential Customer into 1 or 0\
  ‚û°Ô∏è Based on the requirements of the model (eg look into customers who ever purchased after received the ads in last 12 months)
  
2. Data cleaning üí®\
  ‚û°Ô∏è Make sure the features that supposed to be either int64 or object are assigned correctly (eg Cust_Last_Purchase, Cust_Ann_Income should be in int64)\
  ‚û°Ô∏è Check if there's duplications in the dataset & remove if any\
    ‚ú® Also look into the dataset, if got duplicates for the common key üÜî ‚ú®\
  ‚û°Ô∏è Check the data for missing values\
    ‚ú® For some numerical data, missing value can be simply means 0 (eg: Last_Cust_Purchase) ‚ú®\
    ‚ú® There will be situation where 0 won't be the best option (eg: Cust_Age). Thus, we can replace it with the mean if it's not skewed or median if it's skewed ‚ú®\
    ‚ú® For categorical, one of the choice is to remove the data if the count is small ‚ú®
    
3. Exploratory Data Analysis (EDA) üí¶\
  ‚û°Ô∏è Commonly there should be only 2 types of categories for the variables (Categorical & Numerical)\
  ‚û°Ô∏è Create insights to look at the relationship between the variables\
    ‚ú® We should remove variables that is highly correlated with the "Potential Customer" ‚ú®\
      eg: Cust_Tenure (longer the tenure of the customers with the company, the higher change of them to purchase an item)\
    ‚ú®However, not all the time the Cust_Tenure has higher correlation, it actually depending on the objective and requirements of the built model‚ú®
    
4. Feature Engineering & Selection üë∑ \
  ‚û°Ô∏è Imply high level features that reflect the interactions between multiple columns as new features to get better insight and feed more information to the predictive model\
    ‚ú® Eg instead of a numerical column, can use log of the column, square of the column, or any other transformation of the column‚ú®\
  ‚û°Ô∏è We can manually remove the highly correlation features, or can let the PCA handles it during pre-processing
  
5. Data Preprocessing üß∞ \
  ‚û°Ô∏è Seperate X(features) and y(target) \
    ‚ú® y(target) = "Potential_Customer" ‚ú® \
  ‚û°Ô∏è Change categorical variables with numerical variables (One-hot encoding or Dummy encoding) \
  refer here : https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db \
    ‚ú® Can also try to look into the Label Encoding (not being used in the notebook) ‚ú®\
  refer here : https://www.naukri.com/learning/articles/one-hot-encoding-vs-label-encoding/ \
  ‚û°Ô∏è Split data into train/test \
    ‚ú® Split the data into 75% train, 25% test ‚ú®\
    ‚ú® If wanna go more, we can add Over-The-Time(OTT) validation (not being used in the notebook) ‚ú®\
  ‚û°Ô∏è Feature scaling \
    ‚ú® Also can try OverSampling & Undersampling ‚ú®\
  refer here : https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/ \
    ‚ú® Make used of Standard Scaler, Min Max Scaler or Power Transformer ‚ú®\
  refer here : https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35 \
  ‚û°Ô∏è PCA on Numerical Columns only \
    ‚ú® To check the highly correlated features ‚ú® \
  refer here : https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e \
  
6. Machine Learning üíª \
  ‚û°Ô∏è Logistic Regression \
  ‚û°Ô∏è K-Neighbour Network (KNN) \
  ‚û°Ô∏è Decision Tree \
    ‚ú® To check the best ML algorithm, the Precision, Recall, F1 and Accuracy were used ‚ú®\
  refer here : https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9 \
    ‚ú® Confusion Matrix also crucial in the phase of algorithm selection ‚ú® \
  refer here : https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62
